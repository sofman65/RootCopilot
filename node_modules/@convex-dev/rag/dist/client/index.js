import { embed, embedMany, generateText, } from "ai";
import { assert } from "convex-helpers";
import { createFunctionHandle, internalActionGeneric, internalMutationGeneric, } from "convex/server";
import { CHUNK_BATCH_SIZE, filterNamesContain, vChunkerArgs, vEntryId, vNamespaceId, vOnCompleteArgs, } from "../shared.js";
import { defaultChunker } from "./defaultChunker.js";
export { hybridRank } from "./hybridRank.js";
export { defaultChunker, vEntryId, vNamespaceId };
export { vEntry, vOnCompleteArgs, vSearchEntry, vSearchResult, } from "../shared.js";
export { contentHashFromArrayBuffer, guessMimeTypeFromContents, guessMimeTypeFromExtension, } from "./fileUtils.js";
const DEFAULT_SEARCH_LIMIT = 10;
export class RAG {
    component;
    options;
    /**
     * A component to use for Retrieval-Augmented Generation.
     * Create one for each model and embedding dimension you want to use.
     * When migrating between models / embedding lengths, create multiple
     * instances and do your `add`s with the appropriate instance, and searches
     * against the appropriate instance to get results with those parameters.
     *
     * The filterNames need to match the names of the filters you provide when
     * adding and when searching. Use the type parameter to make this type safe.
     *
     * The second type parameter makes the entry metadata type safe. E.g. you can
     * do rag.add(ctx, {
     *   namespace: "my-namespace",
     *   metadata: {
     *     source: "website" as const,
     *   },
     * })
     * and then entry results will have the metadata type `{ source: "website" }`.
     */
    constructor(component, options) {
        this.component = component;
        this.options = options;
    }
    /**
     * Add an entry to the store. It will chunk the text with the `defaultChunker`
     * if you don't provide chunks, and embed the chunks with the default model
     * if you don't provide chunk embeddings.
     *
     * If you provide a key, it will replace an existing entry with the same key.
     * If you don't provide a key, it will always create a new entry.
     * If you provide a contentHash, it will deduplicate the entry if it already exists.
     * The filterValues you provide can be used later to search for it.
     */
    async add(ctx, args) {
        let namespaceId;
        if ("namespaceId" in args) {
            namespaceId = args.namespaceId;
        }
        else {
            const namespace = await this.getOrCreateNamespace(ctx, {
                namespace: args.namespace,
                status: "ready",
            });
            namespaceId = namespace.namespaceId;
        }
        validateAddFilterValues(args.filterValues, this.options.filterNames);
        const chunks = args.chunks ?? defaultChunker(args.text);
        let allChunks;
        const totalUsage = { tokens: 0 };
        if (Array.isArray(chunks) && chunks.length < CHUNK_BATCH_SIZE) {
            const result = await createChunkArgsBatch(this.options.textEmbeddingModel, chunks);
            allChunks = result.chunks;
            totalUsage.tokens += result.usage.tokens;
        }
        const onComplete = args.onComplete && (await createFunctionHandle(args.onComplete));
        const { entryId, status, created } = await ctx.runMutation(this.component.entries.add, {
            entry: {
                key: args.key,
                namespaceId,
                title: args.title,
                metadata: args.metadata,
                filterValues: args.filterValues ?? [],
                importance: args.importance ?? 1,
                contentHash: args.contentHash,
            },
            onComplete,
            allChunks,
        });
        if (status === "ready") {
            return {
                entryId: entryId,
                status,
                created,
                replacedEntry: null,
                usage: totalUsage,
            };
        }
        let isPending = false;
        if (allChunks) {
            // If we added all the chunks and we're here, they're pending.
            isPending = true;
        }
        else {
            // break chunks up into batches, respecting soft limit
            let startOrder = 0;
            for await (const batch of batchIterator(chunks, CHUNK_BATCH_SIZE)) {
                const result = await createChunkArgsBatch(this.options.textEmbeddingModel, batch);
                totalUsage.tokens += result.usage.tokens;
                const { status } = await ctx.runMutation(this.component.chunks.insert, {
                    entryId,
                    startOrder,
                    chunks: result.chunks,
                });
                startOrder += result.chunks.length;
                if (status === "pending") {
                    isPending = true;
                }
            }
        }
        if (isPending) {
            let startOrder = 0;
            // replace any older version of the entry with the new one
            while (true) {
                const { status, nextStartOrder } = await ctx.runMutation(this.component.chunks.replaceChunksPage, { entryId, startOrder });
                if (status === "ready") {
                    break;
                }
                else if (status === "replaced") {
                    return {
                        entryId: entryId,
                        status: "replaced",
                        created: false,
                        replacedEntry: null,
                        usage: totalUsage,
                    };
                }
                startOrder = nextStartOrder;
            }
        }
        const promoted = await ctx.runMutation(this.component.entries.promoteToReady, { entryId });
        return {
            entryId: entryId,
            status: "ready",
            replacedEntry: promoted.replacedEntry,
            created: true,
            usage: totalUsage,
        };
    }
    /**
     * Add an entry to the store asynchronously.
     *
     * This is useful if you want to chunk the entry in a separate process,
     * or if you want to chunk the entry in a separate process.
     *
     * The chunkerAction is a function that splits the entry into chunks and
     * embeds them. It should be passed as internal.foo.myChunkerAction
     * e.g.
     * ```ts
     * export const myChunkerAction = rag.defineChunkerAction(async (ctx, args) => {
     *   // ...
     *   return { chunks: [chunk1, chunk2, chunk3] };
     * });
     *
     * // in your mutation
     *   const entryId = await rag.addAsync(ctx, {
     *     key: "myfile.txt",
     *     namespace: "my-namespace",
     *     chunkerAction: internal.foo.myChunkerAction,
     *   });
     * ```
     */
    async addAsync(ctx, args) {
        let namespaceId;
        if ("namespaceId" in args) {
            namespaceId = args.namespaceId;
        }
        else {
            const namespace = await this.getOrCreateNamespace(ctx, {
                namespace: args.namespace,
                status: "ready",
            });
            namespaceId = namespace.namespaceId;
        }
        validateAddFilterValues(args.filterValues, this.options.filterNames);
        const onComplete = args.onComplete
            ? await createFunctionHandle(args.onComplete)
            : undefined;
        const chunker = await createFunctionHandle(args.chunkerAction);
        const { entryId, status } = await ctx.runMutation(this.component.entries.addAsync, {
            entry: {
                key: args.key,
                namespaceId,
                title: args.title,
                metadata: args.metadata,
                filterValues: args.filterValues ?? [],
                importance: args.importance ?? 1,
                contentHash: args.contentHash,
            },
            onComplete,
            chunker,
        });
        return { entryId: entryId, status };
    }
    /**
     * Search for entries in a namespace with configurable filters.
     * You can provide a query string or target embedding, as well as search
     * parameters to filter and constrain the results.
     */
    async search(ctx, args) {
        const { namespace, filters = [], limit = DEFAULT_SEARCH_LIMIT, chunkContext = { before: 0, after: 0 }, vectorScoreThreshold, } = args;
        let embedding = Array.isArray(args.query) ? args.query : undefined;
        let usage = { tokens: 0 };
        if (!embedding) {
            const embedResult = await embed({
                model: this.options.textEmbeddingModel,
                value: args.query,
            });
            embedding = embedResult.embedding;
            usage = embedResult.usage;
        }
        const { results, entries } = await ctx.runAction(this.component.search.search, {
            embedding,
            namespace,
            modelId: getModelId(this.options.textEmbeddingModel),
            filters,
            limit,
            vectorScoreThreshold,
            chunkContext,
        });
        const entriesWithTexts = entries.map((e) => {
            const ranges = results
                .filter((r) => r.entryId === e.entryId)
                .sort((a, b) => a.startOrder - b.startOrder);
            let text = "";
            let previousEnd = 0;
            for (const range of ranges) {
                if (previousEnd !== 0) {
                    if (range.startOrder !== previousEnd) {
                        text += "\n\n...\n\n";
                    }
                    else {
                        text += "\n";
                    }
                }
                text += range.content.map((c) => c.text).join("\n");
                previousEnd = range.startOrder + range.content.length;
            }
            return { ...e, text };
        });
        return {
            results: results,
            text: entriesWithTexts
                .map((e) => (e.title ? `## ${e.title}:\n\n${e.text}` : e.text))
                .join(`\n\n---\n\n`),
            entries: entriesWithTexts,
            usage,
        };
    }
    /**
     * Generate text based on Retrieval-Augmented Generation.
     *
     * This will search for entries in the namespace based on the prompt and use
     * the results as context to generate text, using the search options args.
     * You can override the default "system" message to provide instructions on
     * using the context and answering in the appropriate style.
     * You can provide "messages" in addition to the prompt to provide
     * extra context / conversation history.
     */
    async generateText(ctx, args) {
        const { search: { namespace, ...searchOpts }, prompt, ...aiSdkOpts } = args;
        const context = await this.search(ctx, {
            namespace,
            query: prompt,
            ...searchOpts,
        });
        let contextHeader = "Use the following context to respond to the user's question:\n";
        let contextContents = context.text;
        let contextFooter = "\n--------------------------------\n";
        let userQuestionHeader = "";
        let userQuestionFooter = "";
        let userPrompt = prompt;
        switch (getModelCategory(aiSdkOpts.model)) {
            case "openai":
                userQuestionHeader = '**User question:**\n"""';
                userQuestionFooter = '"""';
                break;
            case "meta":
                userQuestionHeader = "**User question:**\n";
                break;
            case "google":
                userQuestionHeader = "<question>";
                userQuestionFooter = "</question>";
            // fallthrough
            case "anthropic":
                contextHeader = "<context>";
                contextContents = context.entries
                    .map((e) => e.title
                    ? `<document title="${e.title}">${e.text}</document>`
                    : `<document>${e.text}</document>`)
                    .join("\n");
                contextFooter = "</context>";
                userPrompt = prompt.replace(/</g, "&lt;").replace(/>/g, "&gt;");
                break;
            default:
        }
        const promptWithContext = [
            contextHeader,
            contextContents,
            contextFooter,
            "\n",
            userQuestionHeader,
            userPrompt,
            userQuestionFooter,
        ]
            .join("\n")
            .trim();
        const result = (await generateText({
            system: "You use the context provided only to produce a response. Do not preface the response with acknowledgement of the context.",
            ...aiSdkOpts,
            messages: [
                ...(args.messages ?? []),
                {
                    role: "user",
                    content: promptWithContext,
                },
            ],
        }));
        result.context = context;
        return result;
    }
    /**
     * List all entries in a namespace.
     */
    async list(ctx, args) {
        const paginationOpts = "paginationOpts" in args
            ? args.paginationOpts
            : { cursor: null, numItems: args.limit };
        const results = await ctx.runQuery(this.component.entries.list, {
            namespaceId: args.namespaceId,
            paginationOpts,
            order: args.order ?? "asc",
            status: args.status ?? "ready",
        });
        return results;
    }
    /**
     * Get entry metadata by its id.
     */
    async getEntry(ctx, args) {
        const entry = await ctx.runQuery(this.component.entries.get, {
            entryId: args.entryId,
        });
        return entry;
    }
    /**
     * Find an existing entry by its content hash, which you can use to copy
     * new results into a new entry when migrating, or avoiding duplicating work
     * when updating content.
     */
    async findEntryByContentHash(ctx, args) {
        const entry = await ctx.runQuery(this.component.entries.findByContentHash, {
            namespace: args.namespace,
            dimension: this.options.embeddingDimension,
            filterNames: this.options.filterNames ?? [],
            modelId: getModelId(this.options.textEmbeddingModel),
            key: args.key,
            contentHash: args.contentHash,
        });
        return entry;
    }
    /**
     * Get a namespace that matches the modelId, embedding dimension, and
     * filterNames of the RAG instance. If it doesn't exist, it will be created.
     */
    async getOrCreateNamespace(ctx, args) {
        const onComplete = args.onComplete
            ? await createFunctionHandle(args.onComplete)
            : undefined;
        assert(!onComplete || args.status === "pending", "You can only supply an onComplete handler for pending namespaces");
        const { namespaceId, status } = await ctx.runMutation(this.component.namespaces.getOrCreate, {
            namespace: args.namespace,
            status: args.status ?? "ready",
            onComplete,
            modelId: getModelId(this.options.textEmbeddingModel),
            dimension: this.options.embeddingDimension,
            filterNames: this.options.filterNames ?? [],
        });
        return { namespaceId: namespaceId, status };
    }
    /**
     * Get a namespace that matches the modelId, embedding dimension, and
     * filterNames of the RAG instance. If it doesn't exist, it will return null.
     */
    async getNamespace(ctx, args) {
        return ctx.runQuery(this.component.namespaces.get, {
            namespace: args.namespace,
            modelId: getModelId(this.options.textEmbeddingModel),
            dimension: this.options.embeddingDimension,
            filterNames: this.options.filterNames ?? [],
        });
    }
    /**
     * List all chunks for an entry, paginated.
     */
    async listChunks(ctx, args) {
        return ctx.runQuery(this.component.chunks.list, {
            entryId: args.entryId,
            paginationOpts: args.paginationOpts,
            order: args.order ?? "asc",
        });
    }
    /**
     * Delete an entry and all its chunks in the background using a workpool.
     */
    async deleteAsync(ctx, args) {
        await ctx.runMutation(this.component.entries.deleteAsync, {
            entryId: args.entryId,
            startOrder: 0,
        });
    }
    async delete(ctx, args) {
        if ("runAction" in ctx) {
            await ctx.runAction(this.component.entries.deleteSync, {
                entryId: args.entryId,
            });
        }
        else {
            console.warn("You are running `rag.delete` in a mutation. This is deprecated. Use `rag.deleteAsync` from mutations, or `rag.delete` in actions.");
            await ctx.runMutation(this.component.entries.deleteAsync, {
                entryId: args.entryId,
                startOrder: 0,
            });
        }
    }
    /**
     * Delete all entries with a given key (asynchrounously).
     */
    async deleteByKeyAsync(ctx, args) {
        await ctx.runMutation(this.component.entries.deleteByKeyAsync, {
            namespaceId: args.namespaceId,
            key: args.key,
            beforeVersion: args.beforeVersion,
        });
    }
    /**
     * Delete all entries with a given key (synchronously).
     * If you are getting warnings about `ctx` not being compatible,
     * you're likely running this in a mutation.
     * Use `deleteByKeyAsync` or run `delete` in an action.
     */
    async deleteByKey(ctx, args) {
        await ctx.runAction(this.component.entries.deleteByKeySync, args);
    }
    /**
     * Define a function that can be provided to the `onComplete` parameter of
     * `add` or `addAsync` like:
     * ```ts
     * const onComplete = rag.defineOnComplete(async (ctx, args) => {
     *   // ...
     * });
     *
     * // in your mutation
     *   await rag.add(ctx, {
     *     namespace: "my-namespace",
     *     onComplete: internal.foo.onComplete,
     *   });
     * ```
     * It will be called when the entry is no longer "pending".
     * This is usually when it's "ready" but it can be "replaced" if a newer
     * entry is ready before this one.
     */
    defineOnComplete(fn) {
        return internalMutationGeneric({
            args: vOnCompleteArgs,
            handler: fn,
        });
    }
    /**
     * Define a function that can be provided to the `chunkerAction` parameter of
     * `addAsync` like:
     * ```ts
     * const chunkerAction = rag.defineChunkerAction(async (ctx, args) => {
     *   // ...
     * });
     *
     * // in your mutation
     *   const entryId = await rag.addAsync(ctx, {
     *     key: "myfile.txt",
     *     namespace: "my-namespace",
     *     chunkerAction: internal.foo.myChunkerAction,
     *   });
     * ```
     * It will be called when the entry is added, or when the entry is replaced
     * along the way.
     */
    defineChunkerAction(fn) {
        return internalActionGeneric({
            args: vChunkerArgs,
            handler: async (ctx, args) => {
                const { namespace, entry } = args;
                const modelId = getModelId(this.options.textEmbeddingModel);
                if (namespace.modelId !== modelId) {
                    console.error(`You are using a different embedding model ${modelId} for asynchronously ` +
                        `generating chunks than the one provided when it was started: ${namespace.modelId}`);
                    return;
                }
                if (namespace.dimension !== this.options.embeddingDimension) {
                    console.error(`You are using a different embedding dimension ${this.options.embeddingDimension} for asynchronously ` +
                        `generating chunks than the one provided when it was started: ${namespace.dimension}`);
                    return;
                }
                if (!filterNamesContain(namespace.filterNames, this.options.filterNames ?? [])) {
                    console.error(`You are using a different filters (${this.options.filterNames?.join(", ")}) for asynchronously ` +
                        `generating chunks than the one provided when it was started: ${namespace.filterNames.join(", ")}`);
                    return;
                }
                const chunksPromise = fn(ctx, {
                    namespace,
                    entry: entry,
                });
                let chunkIterator;
                if (chunksPromise instanceof Promise) {
                    const chunks = await chunksPromise;
                    chunkIterator = {
                        [Symbol.asyncIterator]: async function* () {
                            yield* chunks.chunks;
                        },
                    };
                }
                else {
                    chunkIterator = chunksPromise;
                }
                let batchOrder = 0;
                for await (const batch of batchIterator(chunkIterator, CHUNK_BATCH_SIZE)) {
                    const result = await createChunkArgsBatch(this.options.textEmbeddingModel, batch);
                    await ctx.runMutation(args.insertChunks, {
                        entryId: entry.entryId,
                        startOrder: batchOrder,
                        chunks: result.chunks,
                    });
                    batchOrder += result.chunks.length;
                }
            },
        });
    }
}
async function* batchIterator(iterator, batchSize) {
    let batch = [];
    for await (const item of iterator) {
        batch.push(item);
        if (batch.length >= batchSize) {
            yield batch;
            batch = [];
        }
    }
    if (batch.length > 0) {
        yield batch;
    }
}
function validateAddFilterValues(filterValues, filterNames) {
    if (!filterValues) {
        return;
    }
    if (!filterNames) {
        throw new Error("You must provide filter names to RAG to add entries with filters.");
    }
    const seen = new Set();
    for (const filterValue of filterValues) {
        if (seen.has(filterValue.name)) {
            throw new Error(`You cannot provide the same filter name twice: ${filterValue.name}.`);
        }
        seen.add(filterValue.name);
    }
    for (const filterName of filterNames) {
        if (!seen.has(filterName)) {
            throw new Error(`Filter name ${filterName} is not valid (one of ${filterNames.join(", ")}).`);
        }
    }
}
function makeBatches(items, batchSize) {
    const batches = [];
    for (let i = 0; i < items.length; i += batchSize) {
        batches.push(items.slice(i, i + batchSize));
    }
    return batches;
}
async function createChunkArgsBatch(embedModel, chunks) {
    const argsMaybeMissingEmbeddings = chunks.map((chunk) => {
        if (typeof chunk === "string") {
            return { content: { text: chunk } };
        }
        else if ("text" in chunk) {
            const { text, metadata, keywords: searchableText } = chunk;
            return {
                content: { text, metadata },
                embedding: chunk.embedding,
                searchableText,
            };
        }
        else if ("pageContent" in chunk) {
            const { pageContent: text, metadata, keywords: searchableText } = chunk;
            return {
                content: { text, metadata },
                embedding: chunk.embedding,
                searchableText,
            };
        }
        else {
            throw new Error("Invalid chunk: " + JSON.stringify(chunk));
        }
    });
    const missingEmbeddingsWithIndex = argsMaybeMissingEmbeddings
        .map((arg, index) => arg.embedding
        ? null
        : {
            text: arg.content.text,
            index,
        })
        .filter((b) => b !== null);
    const totalUsage = { tokens: 0 };
    for (const batch of makeBatches(missingEmbeddingsWithIndex, 100)) {
        const { embeddings, usage } = await embedMany({
            model: embedModel,
            values: batch.map((b) => b.text.trim() || "<empty>"),
        });
        totalUsage.tokens += usage.tokens;
        for (const [index, embedding] of embeddings.entries()) {
            argsMaybeMissingEmbeddings[batch[index].index].embedding = embedding;
        }
    }
    const finalChunks = argsMaybeMissingEmbeddings.filter((a) => {
        if (a.embedding === undefined) {
            throw new Error("Embedding is undefined for chunk " + a.content.text);
        }
        return true;
    });
    return { chunks: finalChunks, usage: totalUsage };
}
function getModelCategory(model) {
    if (typeof model !== "string") {
        return model.provider;
    }
    if (model.startsWith("openai") ||
        model.startsWith("gpt") ||
        model.startsWith("o1")) {
        return "openai";
    }
    if (model.startsWith("anthropic") || model.startsWith("claude")) {
        return "anthropic";
    }
    if (model.startsWith("gemini") || model.startsWith("gemma")) {
        return "google";
    }
    if (model.startsWith("ollama")) {
        return "meta";
    }
    if (model.startsWith("grok")) {
        return "xai";
    }
    return model;
}
export function getModelId(embeddingModel) {
    if (typeof embeddingModel === "string") {
        if (embeddingModel.includes("/")) {
            return embeddingModel.split("/").slice(1).join("/");
        }
        return embeddingModel;
    }
    return "modelId" in embeddingModel
        ? embeddingModel.modelId
        : embeddingModel.model;
}
export function getProviderName(embeddingModel) {
    if (typeof embeddingModel === "string") {
        return embeddingModel.split("/").at(0);
    }
    return embeddingModel.provider;
}
//# sourceMappingURL=index.js.map